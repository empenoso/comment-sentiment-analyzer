#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
======================================

–°–∫—Ä–∏–ø—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç JSON —Ñ–∞–π–ª—ã –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π, –Ω–∞—Ö–æ–¥–∏—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ
–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å RuBERT, –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ –µ–¥–∏–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
–¥–ª—è –∫–∞–∂–¥–æ–π –∏—Å—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.

–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –ø—Ä–æ–µ–∫—Ç–∞ –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω —Ç—É—Ç: https://github.com/empenoso/comment-sentiment-analyzer

–ê–≤—Ç–æ—Ä: –ú–∏—Ö–∞–∏–ª –®–∞—Ä–¥–∏–Ω https://shardin.name/
–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: 04.10.2025
–í–µ—Ä—Å–∏—è: 1.0
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, DefaultDict
from collections import defaultdict

try:
    import torch
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫: {e}", file=sys.stderr)
    print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—â–µ–Ω –≤ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:", file=sys.stderr)
    print("pip install transformers sentencepiece torch", file=sys.stderr)
    sys.exit(1)

# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================================

class Config:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ —Ñ–∞–π–ª–∞ config.env"""

    def __init__(self):
        self.load_env_file()

        self.MODEL_NAME = os.getenv('MODEL_NAME', 'cointegrated/rubert-tiny-sentiment-balanced')
        self.DEVICE = os.getenv('DEVICE', 'cuda' if torch.cuda.is_available() else 'cpu')
        self.POSITIVE_THRESHOLD = float(os.getenv('POSITIVE_THRESHOLD', '1.0'))
        
        exclude_str = os.getenv('EXCLUDE_AUTHORS', 'empenoso,–ú–∏—Ö–∞–∏–ª –®–∞—Ä–¥–∏–Ω')
        exclude_str = exclude_str.strip('"').strip("'")
        self.EXCLUDE_AUTHORS: set[str] = {author.strip() for author in exclude_str.split(',') if author.strip()}
        
        self.MIN_COMMENT_LENGTH = int(os.getenv('MIN_COMMENT_LENGTH', '20'))
        self.MAX_TOKEN_LENGTH = int(os.getenv('MAX_TOKEN_LENGTH', '512'))

        self.INPUT_DIRS: List[Path] = [
            Path('./habr_comments'),
            Path('./smart-lab_comments'),
            Path('./t-j_comments')
        ]
        self.OUTPUT_DIR = Path('./positive_comments') 
        self.CACHE_DIR = Path.home() / '.cache' / 'huggingface'

    def load_env_file(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ config.env"""
        env_file = Path('./config.env')
        if env_file.exists():
            with open(env_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        value = value.strip().strip('"').strip("'")
                        os.environ[key.strip()] = value

    def print_config(self):
        """–í—ã–≤–æ–¥ —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        print("\n" + "="*70)
        print("‚öôÔ∏è  –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø")
        print("="*70)
        print(f"ü§ñ –ú–æ–¥–µ–ª—å:              {self.MODEL_NAME}")
        print(f"üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:          {self.DEVICE}")
        print(f"üìä –ü–æ—Ä–æ–≥ –ø–æ–∑–∏—Ç–∏–≤–Ω–æ—Å—Ç–∏:  {self.POSITIVE_THRESHOLD}")
        print(f"üë§ –ò—Å–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ—Ä–æ–≤:   {list(self.EXCLUDE_AUTHORS)}")
        print(f"üìè –ú–∏–Ω. –¥–ª–∏–Ω–∞:          {self.MIN_COMMENT_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üìÇ –í—Ö–æ–¥–Ω—ã–µ –ø–∞–ø–∫–∏:       {[str(d) for d in self.INPUT_DIRS]}")
        print("="*70 + "\n")

# ============================================================================
# –ê–ù–ê–õ–ò–ó–ê–¢–û–† –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò
# ============================================================================

class SentimentAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ RuBERT"""
    def __init__(self, config: Config):
        self.config = config
        self.tokenizer = None
        self.model = None
        self.load_model()

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.config.MODEL_NAME}...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.MODEL_NAME, cache_dir=self.config.CACHE_DIR)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.config.MODEL_NAME, cache_dir=self.config.CACHE_DIR)
            
            if self.config.DEVICE == 'cuda' and torch.cuda.is_available():
                self.model.to('cuda')
                print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ GPU: {torch.cuda.get_device_name(0)}")
            else:
                self.config.DEVICE = 'cpu'
                print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}", file=sys.stderr)
            sys.exit(1)

    def is_positive_comment(self, text: str, author: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–º"""
        if author in self.config.EXCLUDE_AUTHORS:
            return False
            
        if len(text) < self.config.MIN_COMMENT_LENGTH:
            return False

        with torch.no_grad():
            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=self.config.MAX_TOKEN_LENGTH).to(self.model.device)
            proba = torch.sigmoid(self.model(**inputs).logits).cpu().numpy()[0]

        label = self.model.config.id2label[proba.argmax()]
        
        if label == 'positive' and proba[2] >= self.config.POSITIVE_THRESHOLD:
            return True
        
        praise_keywords = [
            '—Å–ø–∞—Å–∏–±–æ', '–±–ª–∞–≥–æ–¥–∞—Ä—é', '–æ—Ç–ª–∏—á–Ω–æ', '–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ', '–≤–µ–ª–∏–∫–æ–ª–µ–ø–Ω–æ',
            '—Ö–æ—Ä–æ—à–æ', '–º–æ–ª–æ–¥–µ—Ü', '–±—Ä–∞–≤–æ', '—Å—É–ø–µ—Ä', '–∫–ª–∞—Å—Å', '–∫—Ä—É—Ç–æ', '–≤–æ—Å—Ö–∏—â–µ–Ω', '–≤–ø–µ—á–∞—Ç–ª–µ–Ω',
            '–ø–æ–ª–µ–∑–Ω–æ', '–ø–æ–º–æ–≥–ª–æ', '–æ—Ç–ª–∏—á–Ω–∞—è —Å—Ç–∞—Ç—å—è', '—Ö–æ—Ä–æ—à–∞—è —Ä–∞–±–æ—Ç–∞'
        ]
        if label == 'neutral' and any(keyword in text.lower() for keyword in praise_keywords):
            return True
            
        return False

# ============================================================================
# –û–ë–†–ê–ë–û–¢–ö–ê –§–ê–ô–õ–û–í
# ============================================================================

def get_comment_url(directory_name: str, article_url: str, comment_id: str) -> str:
    """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ URL –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    if 'habr' in directory_name.lower():
        # –î–ª—è Habr –∏–∑–≤–ª–µ–∫–∞–µ–º ID —Å—Ç–∞—Ç—å–∏ –∏–∑ URL
        if 'habr.com' in article_url:
            article_id = article_url.split('/articles/')[-1].split('/')[0]
            return f"https://habr.com/ru/articles/{article_id}/comments/#comment_{comment_id}"
        return f"Unknown Habr URL: {article_url}"
    
    elif 'smart-lab' in directory_name.lower():
        # –î–ª—è Smart-Lab –∏–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL
        if 'smart-lab.ru' in article_url:
            article_id = article_url.split('/blog/')[-1].split('.php')[0]
            return f"https://smart-lab.ru/blog/{article_id}.php#comment{comment_id}"
        return f"Unknown Smart-Lab URL: {article_url}"
    
    elif 't-j' in directory_name.lower() or 'tj' in directory_name.lower():
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –¥–ª—è TJ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π URL + #c{comment_id}
        if article_url:
            # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ª—ç—à –∏ —è–∫–æ—Ä—å
            base_url = article_url.rstrip('/').split('#')[0]
            return f"{base_url}/#c{comment_id}"
        return f"Unknown TJ URL (comment: {comment_id})"
    
    else:
        return f"Unknown source (url: {article_url}, comment: {comment_id})"

def process_json_file(file_path: Path, analyzer: SentimentAnalyzer, directory_name: str) -> List[Dict]:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ JSON —Ñ–∞–π–ª–∞"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path.name}: {e}")
        return []

    # –ò–°–ü–†–ê–í–õ–ï–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π URL —Å—Ç–∞—Ç—å–∏
    article_url = data.get('url', '')
    
    comments = data.get('comments', [])
    positive_comments = []

    for comment in comments:
        text = comment.get('text', '')
        author = comment.get('author', '')
        
        if not text or not author:
            continue

        if analyzer.is_positive_comment(text, author):
            comment['article_url'] = article_url  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π URL
            comment['directory_name'] = directory_name
            positive_comments.append(comment)
            
    return positive_comments

# ============================================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ============================================================================

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞"""
    print("\n" + "="*70)
    print("üí¨ –ê–ù–ê–õ–ò–ó–ê–¢–û–† –¢–û–ù–ê–õ–¨–ù–û–°–¢–ò –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ï–í")
    print("="*70)

    config = Config()
    config.print_config()
    
    grouped_positive_comments: DefaultDict[Path, List] = defaultdict(list)
    total_files_to_process = 0
    
    # --- –°–±–æ—Ä –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ ---
    for directory in config.INPUT_DIRS:
        if directory.exists() and directory.is_dir():
            found = list(directory.glob('*.json'))
            if found:
                print(f"üîç –í '{directory}' –Ω–∞–π–¥–µ–Ω–æ {len(found)} json-—Ñ–∞–π–ª–æ–≤.")
                total_files_to_process += len(found)
            else:
                print(f"‚ÑπÔ∏è  –í '{directory}' json-—Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        else:
            print(f"‚ö†Ô∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è '{directory}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")

    if total_files_to_process == 0:
        print("\n‚ùå –í —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ JSON —Ñ–∞–π–ª–∞!", file=sys.stderr)
        sys.exit(1)
        
    print(f"\n‚û°Ô∏è  –í—Å–µ–≥–æ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_files_to_process} —Ñ–∞–π–ª–æ–≤.")

    analyzer = SentimentAnalyzer(config)

    print("\n" + "="*70)
    print("üöÄ –ù–ê–ß–ê–õ–û –ê–ù–ê–õ–ò–ó–ê")
    print("="*70 + "\n")

    # --- –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ ---
    processed_files = 0
    total_positive = 0
    for directory in config.INPUT_DIRS:
        if not directory.is_dir():
            continue
        
        for file_path in directory.glob('*.json'):
            processed_files += 1
            print(f"\r[{processed_files}/{total_files_to_process}] –û–±—Ä–∞–±–æ—Ç–∫–∞ {file_path.name}...", end='')
            
            positive_comments = process_json_file(file_path, analyzer, directory.name)
            
            if positive_comments:
                grouped_positive_comments[directory].extend(positive_comments)
                total_positive += len(positive_comments)
    
    # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---
    print("\n\n" + "="*70)
    print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*70)
    
    if not grouped_positive_comments:
         print("‚ÑπÔ∏è  –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
    else:
        for directory, comments in grouped_positive_comments.items():
            output_file = directory / f"{directory.name}_positive_comments.txt"
            print(f"üìù –°–æ—Ö—Ä–∞–Ω—è—é {len(comments)} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤ —Ñ–∞–π–ª: {output_file}")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –ø–æ ID –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            comments.sort(key=lambda c: c.get('id') if c.get('id') is not None else 0)

            with open(output_file, 'w', encoding='utf-8') as f:
                for idx, comment in enumerate(comments):
                    author = comment.get('author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä')
                    datetime = comment.get('datetime', '–î–∞—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞')
                    text = comment.get('text', '')
                    comment_id = comment.get('id', 'unknown')
                    article_url = comment.get('article_url', '')  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π URL
                    directory_name = comment.get('directory_name', '')
                    
                    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø–µ—Ä–µ–¥–∞–µ–º article_url –≤–º–µ—Å—Ç–æ article_id
                    comment_url = get_comment_url(directory_name, article_url, str(comment_id))
                    
                    # –ö—Ä–∞—Å–∏–≤–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
                    f.write(f"–ê–≤—Ç–æ—Ä: {author}\n")
                    f.write(f"–î–∞—Ç–∞: {datetime}\n")
                    f.write(f"–¢–µ–∫—Å—Ç: {text}\n")
                    f.write(f"–°—Å—ã–ª–∫–∞: {comment_url}\n")
                    
                    # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –º–µ–∂–¥—É –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ (100 –∑–Ω–∞–∫–æ–≤ '=')
                    if idx < len(comments) - 1:
                        f.write("=" * 100 + "\n")

    print("\n" + "="*70)
    print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("="*70)
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤:          {processed_files}")
    print(f"–ù–∞–π–¥–µ–Ω–æ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö:         {total_positive}")
    
    if total_positive > 0:
        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö.")
    else:
        print("‚ÑπÔ∏è  –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å `POSITIVE_THRESHOLD` –≤ `config.env`.")
    print("="*70 + "\n")


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –ü—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.", file=sys.stderr)
        sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)